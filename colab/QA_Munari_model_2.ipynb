{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QA_Munari_model_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhB1txNs--qG"
      },
      "source": [
        "from google.colab import drive\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import keras.backend as K\n",
        "\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "proj_dir = \"/content/drive/MyDrive/QA_project\" #Change to directory where there are the train and dev set and glove embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_RLXkU5pQib"
      },
      "source": [
        "!unzip \"/content/drive/MyDrive/QA_project/glove.6B.zip\" -d \"/content\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OXkY6uNDA7u"
      },
      "source": [
        "with open('/content/drive/MyDrive/QA_project/train-v2.0.json') as f:\n",
        "  train = json.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/QA_project/dev-v2.0.json') as f:\n",
        "  dev = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBgcqTz9jw5I"
      },
      "source": [
        "# Load train data\n",
        "train_contexts = []\n",
        "train_questions = []\n",
        "train_questions_id = [] #used for evaluation script\n",
        "train_is_imp = []\n",
        "train_answers = []\n",
        "train_answers_start = []\n",
        "train_answers_end = []\n",
        "train_index = []\n",
        "\n",
        "i = 0\n",
        "\n",
        "for data in train['data']:\n",
        "  title = data['title']\n",
        "\n",
        "  for paragraph in data['paragraphs']:\n",
        "    context = paragraph['context']\n",
        "\n",
        "    for qa in paragraph['qas']:\n",
        "      question = qa['question']\n",
        "      id = qa['id']\n",
        "      is_impossible = qa['is_impossible']\n",
        "\n",
        "      if not is_impossible:\n",
        "        for answer in qa['answers']:\n",
        "          text = answer['text']\n",
        "          start = answer['answer_start']\n",
        "          \n",
        "          train_is_imp.append(0)\n",
        "          train_contexts.append(context)\n",
        "          train_questions.append(question)\n",
        "          train_questions_id.append(id)\n",
        "          train_answers.append(text)\n",
        "          train_answers_start.append(start)\n",
        "          train_answers_end.append(start + len(text))\n",
        "          train_index.append(i)\n",
        "          i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXZCyXQQENl3"
      },
      "source": [
        "# Load dev data\n",
        "dev_contexts = []\n",
        "dev_questions = []\n",
        "dev_questions_id = [] #used for evaluation script\n",
        "dev_is_imp = []\n",
        "dev_answers = []\n",
        "dev_answers_start = []\n",
        "dev_answers_end = []\n",
        "\n",
        "for data in dev['data']:\n",
        "  title = data['title']\n",
        "\n",
        "  for paragraph in data['paragraphs']:\n",
        "    context = paragraph['context']\n",
        "\n",
        "    for qas in paragraph['qas']:\n",
        "      question = qas['question']\n",
        "      id = qas['id']\n",
        "      is_impossible = qas['is_impossible']\n",
        "\n",
        "      if not is_impossible:\n",
        "        for answer in qas['answers']:\n",
        "          text = answer['text']\n",
        "          start = answer['answer_start']\n",
        "\n",
        "          dev_is_imp.append(0)\n",
        "          dev_contexts.append(context)\n",
        "          dev_questions.append(question)\n",
        "          dev_questions_id.append(id)\n",
        "          dev_answers.append(text)\n",
        "          dev_answers_start.append(start)\n",
        "          dev_answers_end.append(start + len(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B70ZUumVe2TT"
      },
      "source": [
        "# Tokenizer\n",
        "from nltk.tokenize.regexp import RegexpTokenizer\n",
        "\n",
        "tkn = RegexpTokenizer('[\\s!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\nâ€”]', gaps=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCYxQNIajSZJ"
      },
      "source": [
        "# Tokenize training set and build vocabulary\n",
        "vocab = set()\n",
        "\n",
        "tokenized_train_contexts = tkn.tokenize_sents(map(lambda x:x.lower(),train_contexts))\n",
        "span_train_contexts = list(tkn.span_tokenize_sents(map(lambda x:x.lower(),train_contexts)))\n",
        "\n",
        "tokenized_train_questions = tkn.tokenize_sents(map(lambda x:x.lower(),train_questions))\n",
        "\n",
        "for item in tokenized_train_contexts:\n",
        "  vocab.update(item)\n",
        "\n",
        "for item in tokenized_train_questions:\n",
        "  vocab.update(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uQRZu51V-xD"
      },
      "source": [
        "# Tokenize dev set\n",
        "tokenized_dev_contexts = tkn.tokenize_sents(map(lambda x:x.lower(),dev_contexts))\n",
        "span_dev_contexts = list(tkn.span_tokenize_sents(map(lambda x:x.lower(),dev_contexts)))\n",
        "\n",
        "tokenized_dev_questions = tkn.tokenize_sents(map(lambda x:x.lower(),dev_questions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEz3lgllM7em"
      },
      "source": [
        "# Build word index\n",
        "word_index = {}\n",
        "for idx, voc in enumerate(vocab):\n",
        "  word_index[voc] = idx + 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oxuKGwzO4b_"
      },
      "source": [
        "# Build token-answer correspondence training\n",
        "train_start = []\n",
        "train_end = []\n",
        "\n",
        "for answ_start, answ_end, span_context, is_imp in zip(train_answers_start, train_answers_end, span_train_contexts, train_is_imp):\n",
        "  if is_imp == 1:\n",
        "    train_start.append([0]*len(span_context))\n",
        "    train_end.append([0]*len(span_context))\n",
        "  else:\n",
        "    answer_enc = []\n",
        "    answer_start_enc = []\n",
        "    answer_end_enc = [0]*len(span_context)\n",
        "\n",
        "    started = False\n",
        "    end_idx = None\n",
        "\n",
        "    for idx, span in enumerate(span_context):\n",
        "      if span[0] >= answ_start and not started:\n",
        "        answer_start_enc.append(1)\n",
        "        answer_enc.append(1)\n",
        "        started = True\n",
        "        end_idx = idx\n",
        "      elif started and span[1] <= answ_end:\n",
        "        end_idx = idx\n",
        "        answer_start_enc.append(0)\n",
        "        answer_enc.append(1)\n",
        "      else:\n",
        "        answer_start_enc.append(0)\n",
        "        answer_enc.append(0)\n",
        "\n",
        "    if end_idx:\n",
        "      answer_end_enc[end_idx] = 1\n",
        "\n",
        "    train_start.append(answer_start_enc)\n",
        "    train_end.append(answer_end_enc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tELUl0xWRUG"
      },
      "source": [
        "# Build token-answer correspondence dev\n",
        "dev_start = []\n",
        "dev_end = []\n",
        "\n",
        "for answ_start, answ_end, span_context, is_imp in zip(dev_answers_start, dev_answers_end, span_dev_contexts, dev_is_imp):\n",
        "  if is_imp == 1:\n",
        "    dev_start.append([0]*len(span_context))\n",
        "    dev_end.append([0]*len(span_context))\n",
        "  else:\n",
        "    answer_enc = []\n",
        "    answer_start_enc = []\n",
        "    answer_end_enc = [0]*len(span_context)\n",
        "\n",
        "    started = False\n",
        "    end_idx = None\n",
        "\n",
        "    for idx, span in enumerate(span_context):\n",
        "      if span[0] >= answ_start and not started:\n",
        "        answer_start_enc.append(1)\n",
        "        answer_enc.append(1)\n",
        "        started = True\n",
        "        end_idx = idx\n",
        "      elif started and span[1] <= answ_end:\n",
        "        end_idx = idx\n",
        "        answer_start_enc.append(0)\n",
        "        answer_enc.append(1)\n",
        "      else:\n",
        "        answer_start_enc.append(0)\n",
        "        answer_enc.append(0)\n",
        "\n",
        "    if end_idx:\n",
        "      answer_end_enc[end_idx] = 1\n",
        "\n",
        "    dev_start.append(answer_start_enc)\n",
        "    dev_end.append(answer_end_enc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up-p5L4t1oMn"
      },
      "source": [
        "del span_train_contexts\n",
        "del span_dev_contexts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hQFlSKhXBz6"
      },
      "source": [
        "def tokenized_texts_to_sequences(tkn_texts, word_index):\n",
        "  sequences = []\n",
        "  for text in tkn_texts:\n",
        "    seq = []\n",
        "    for word in text:\n",
        "      seq.append(word_index[word]) if word in word_index.keys() else seq.append(1)\n",
        "    sequences.append(seq)\n",
        "  \n",
        "  return sequences\n",
        "\n",
        "# Integer encoding training\n",
        "train_ctx = tokenized_texts_to_sequences(tokenized_train_contexts, word_index)\n",
        "train_q = tokenized_texts_to_sequences(tokenized_train_questions, word_index)\n",
        "\n",
        "# Integer encoding dev\n",
        "dev_ctx = tokenized_texts_to_sequences(tokenized_dev_contexts, word_index)\n",
        "dev_q = tokenized_texts_to_sequences(tokenized_dev_questions, word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mB-hhWwbK0a"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Pad train sequences and answer encoding\n",
        "train_ctx = pad_sequences(train_ctx, padding='post')\n",
        "train_q = pad_sequences(train_q, padding='post')\n",
        "train_start = pad_sequences(train_start, padding='post')\n",
        "train_end = pad_sequences(train_end, padding='post')\n",
        "\n",
        "# Pad dev sequences\n",
        "dev_ctx = pad_sequences(dev_ctx, padding='post')\n",
        "dev_q = pad_sequences(dev_q, padding='post')\n",
        "dev_start = pad_sequences(dev_start, padding='post')\n",
        "dev_end = pad_sequences(dev_end, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy74ktABbfrZ"
      },
      "source": [
        "# Save target list to numpy array\n",
        "train_is_imp_arr = np.array(train_is_imp)\n",
        "\n",
        "dev_is_imp_arr = np.array(dev_is_imp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJP0ara6kl-D"
      },
      "source": [
        "# shuffle train data\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "shuffler = np.random.permutation(train_ctx.shape[0])\n",
        "train_ctx_shuffled = train_ctx[shuffler]\n",
        "train_q_shuffled = train_q[shuffler]\n",
        "train_start_shuffled = train_start[shuffler]\n",
        "train_end_shuffled = train_end[shuffler]\n",
        "train_imp_shuffled = train_is_imp_arr[shuffler]\n",
        "\n",
        "train_index = np.array(train_index)\n",
        "train_index_shuffled = train_index[shuffler]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOTpgwmGpyWU"
      },
      "source": [
        "# Preparing embedding\n",
        "embeddings_index = {}\n",
        "with open('/content/glove.6B.300d.txt') as f:\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.array(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPAtgwh2qnLK"
      },
      "source": [
        "# Define embedding matrix\n",
        "embedding_matrix = np.zeros((len(word_index) + 2, 300))\n",
        "for word, i in word_index.items():\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCxf-ZS8V2-B"
      },
      "source": [
        "num_words = len(word_index) + 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMMsLljAkdI5"
      },
      "source": [
        "MODEL DEFINITION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1wXO67XcQg0"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Sequential\n",
        "from tensorflow.keras.layers import *\n",
        "import keras.backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbHlzZmw021t"
      },
      "source": [
        "class RepeatConcatMult(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(RepeatConcatMult, self).__init__()\n",
        "\n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "    if mask:\n",
        "      return mask[0]\n",
        "    return mask\n",
        "\n",
        "  def call(self, inputs, training, mask=None):\n",
        "    ctx = inputs[0]\n",
        "    q = inputs[1]\n",
        "\n",
        "    q_repeated = K.tile(q, [1,K.shape(ctx)[1],1])\n",
        "\n",
        "    return K.concatenate([K.concatenate([q_repeated, ctx], axis=-1), tf.multiply(q_repeated,ctx)])\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config().copy()\n",
        "        \n",
        "    return config\n",
        "\n",
        "  @classmethod\n",
        "  def from_config(cls, config):\n",
        "    return cls(**config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42PgDKQBcTHh"
      },
      "source": [
        "# define input layers\n",
        "question_input = keras.Input(shape=(None,), name=\"question\")\n",
        "context_input = keras.Input(shape=(None,), name=\"context\")\n",
        "\n",
        "# embedding\n",
        "token_emb = Embedding(num_words, 300, weights=[embedding_matrix], trainable=False, mask_zero=True)\n",
        "q_emb = token_emb(question_input)\n",
        "q_emb = Dropout(0.4)(q_emb)\n",
        "c_emb = token_emb(context_input)\n",
        "c_emb = Dropout(0.4)(c_emb)\n",
        "\n",
        "# encoder\n",
        "q_encoding = Bidirectional(GRU(units=64, return_sequences=True, dropout=0.1))(q_emb)\n",
        "c_encoding = Bidirectional(GRU(units=64, return_sequences=True, dropout=0.1))(c_emb)\n",
        "\n",
        "# combination\n",
        "q_pool = GlobalAveragePooling1D(keepdims=True)(q_encoding)\n",
        "concat = RepeatConcatMult()([c_encoding, q_pool])\n",
        "\n",
        "# prediction\n",
        "dense = TimeDistributed(Dense(100,activation='relu'))(concat)\n",
        "dense = Dropout(0.2)(dense)\n",
        "dense = TimeDistributed(Dense(50,activation='relu'))(dense)\n",
        "dense = Dropout(0.2)(dense)\n",
        "\n",
        "start_predictor = TimeDistributed(Dense(10,activation='relu'))(dense)\n",
        "start_predictor = Dropout(0.1)(start_predictor)\n",
        "start_predictor = TimeDistributed(Dense(units=1, activation='linear'))(start_predictor)\n",
        "start_predictor = Reshape((-1,))(start_predictor)\n",
        "start_predictor = Softmax(axis=-1, name=\"start\")(start_predictor)\n",
        "\n",
        "end_predictor = TimeDistributed(Dense(10,activation='relu'))(dense)\n",
        "end_predictor = Dropout(0.1)(end_predictor)\n",
        "end_predictor = TimeDistributed(Dense(units=1, activation='linear'))(end_predictor)\n",
        "end_predictor = Reshape((-1,))(end_predictor)\n",
        "end_predictor = Softmax(axis=-1, name=\"end\")(end_predictor)\n",
        "\n",
        "model = keras.Model(\n",
        "    inputs=[question_input, context_input],\n",
        "    outputs=[start_predictor, end_predictor]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjP89oEkcFL_"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss={\"start\": keras.losses.CategoricalCrossentropy(),\n",
        "          \"end\": keras.losses.CategoricalCrossentropy()\n",
        "    }\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRUCQi4Irqaa"
      },
      "source": [
        "keras.utils.plot_model(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BG_cEnplmYx"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O_65ziZjqs1"
      },
      "source": [
        "# apply early stopping\n",
        "import time \n",
        "\n",
        "logdir = os.path.join(os.curdir, \"logs\", \"run_{}\".format(time.time()))\n",
        "\n",
        "callbacks = [keras.callbacks.TensorBoard(logdir),\n",
        "             keras.callbacks.EarlyStopping(patience=5),\n",
        "             keras.callbacks.ModelCheckpoint(\"qa_attention_model.h5\",save_best_only=True)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dr2g6Jsf7Qs"
      },
      "source": [
        "max = 86821\n",
        "history = model.fit(x={\"context\": train_ctx_shuffled, \"question\": train_q_shuffled},\n",
        "                   y={\"start\":train_start_shuffled, \"end\": train_end_shuffled},\n",
        "                   epochs=30,\n",
        "                   batch_size=512,\n",
        "                   validation_split=0.2,\n",
        "                   callbacks=callbacks\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPSCVU1dZVxQ"
      },
      "source": [
        "prediction = model.predict([dev_q, dev_ctx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJTjBVvna85g"
      },
      "source": [
        "def answer(start, end, tokens):\n",
        "  return tokens[start:end + 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMoxnMC5fdnA"
      },
      "source": [
        "preds = {}\n",
        "preds_list = {}\n",
        "for idx in range(len(prediction[0])):\n",
        "  example = prediction[0][idx]\n",
        "  pred_start = np.argmax(example)\n",
        "\n",
        "  example = prediction[1][idx]\n",
        "  pred_end = np.argmax(example)\n",
        "\n",
        "  pred_ans = answer(pred_start, pred_end, tokenized_dev_contexts[idx])\n",
        "\n",
        "  id = dev_questions_id[idx]\n",
        "  preds[id] = ' '.join(pred_ans)\n",
        "  preds_list[id] = pred_ans\n",
        "\n",
        "with open('/content/drive/MyDrive/QA_project/preds.json', 'w') as f:\n",
        "  json.dump(preds, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/QA_project/preds_list.json', 'w') as f:\n",
        "  json.dump(preds_list, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIJ-Fm5Mv_JL"
      },
      "source": [
        "np.save('/content/drive/MyDrive/QA_project/start_token.npy', prediction[0])\n",
        "np.save('/content/drive/MyDrive/QA_project/end_token.npy', prediction[1])\n",
        "np.save('/content/drive/MyDrive/QA_project/scores.npy', prediction[2])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}